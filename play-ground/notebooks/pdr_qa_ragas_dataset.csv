question,answer,contexts,ground_truths
What is the focus of the paper 'A Survey on Retrieval-Augmented Text Generation'?,"The focus of the paper 'A Survey on Retrieval-Augmented Text Generation' is to conduct a survey about retrieval-augmented text generation, highlighting the generic paradigm of retrieval-augmented generation and reviewing notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing']","[""The focus of the paper 'A Survey on Retrieval-Augmented Text Generation' is to provide a comprehensive overview of the field of retrieval-augmented text generation, which has gained significant attention in the computational linguistics community. The paper highlights the generic paradigm of retrieval-augmented generation models, reviews notable approaches across various natural language processing tasks such as dialogue response generation and machine translation, and discusses the state-of-the-art performance achieved by these models. Additionally, the paper identifies and suggests important future research directions in this area.""]"
What is the title of the paper?,"The title of the paper is ""A Survey on Retrieval-Augmented Text Generation"".","['Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 4008–4020, Dublin, Ireland. Association for\nComputational Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul N Bennett. 2021.\nZero-shot dense retrieval with momentum adversar-\nial domain invariant representations. arXiv preprint\narXiv:2110.07581.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv\npreprint arXiv:2007.00808.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018.'
 'and VERiﬁcation.\nIn Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 809–819, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the BIOASQ large-\nscale biomedical semantic indexing and question an-\nswering competition. BMC bioinformatics, 16(1):1–\n28.\nEllen Voorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R. Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: Constructing a pandemic informa-\ntion retrieval test collection. SIGIR Forum, 54(1).\nEllen M Voorhees et al. 2004. Overview of the trec\n2004 robust retrieval track. In Trec, pages 69–77.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018. Retrieval of the best counterargument without\nprior topic knowledge. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 241–251,\nMelbourne, Australia. Association for Computational\nLinguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verifying\nscientiﬁc claims. In Proceedings of the 2020 Con-']","[""The title of the paper is 'A Survey on Retrieval-Augmented Text Generation'.""]"
What is the main focus of this paper?,"The main focus of this paper is to improve the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings']","['The main focus of this paper is to conduct a survey on retrieval-augmented text generation, highlighting its advantages and state-of-the-art performance in various NLP tasks, reviewing notable approaches in tasks such as dialogue response generation and machine translation, and discussing future research directions.']"
What is the topic of the paper 'A Survey on Retrieval-Augmented Text Generation'?,The topic of the paper 'A Survey on Retrieval-Augmented Text Generation' is retrieval-augmented text generation.,"['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","[""The topic of the paper 'A Survey on Retrieval-Augmented Text Generation' is the exploration and analysis of retrieval-augmented text generation methods in computational linguistics. It discusses the advantages of these methods over conventional generation models and their state-of-the-art performance in various NLP tasks. The paper reviews notable approaches to retrieval-augmented text generation, particularly in the context of dialogue response generation, machine translation, and other generation tasks, and suggests important future research directions.""]"
What is the focus of this paper?,"The focus of this paper is on improving the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'for neural conversation generation. In ACL, pages\n1329–1338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498–503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a comprehensive survey of the paradigm, review notable approaches across various NLP tasks such as dialogue response generation and machine translation, and suggest important future research directions.']"
What is the focus of this paper?,"The focus of this paper is on improving the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'for neural conversation generation. In ACL, pages\n1329–1338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498–503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a survey of the current state of retrieval-augmented text generation, highlighting its advantages and reviewing notable approaches across various NLP tasks such as dialogue response generation, machine translation, and other generation tasks. The paper also identifies important future research directions in this area.']"
What is the focus of this paper?,"The focus of this paper is on improving the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'for neural conversation generation. In ACL, pages\n1329–1338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498–503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a comprehensive survey of the current state-of-the-art retrieval-augmented generation models, their advantages, and their applications in various natural language processing tasks such as dialogue response generation, machine translation, and other generation tasks. Additionally, the paper discusses future research directions in this area.']"
What is the purpose of this paper?,"The purpose of this paper is to improve the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora. It aims to develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. The paper also highlights the necessity of augmenting with a mixture of memory for robust generalization and the benefits of augmentation learning.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","['The purpose of this paper is to conduct a comprehensive survey on retrieval-augmented text generation, highlighting its advantages and state-of-the-art performance in various NLP tasks. The paper outlines the generic paradigm of retrieval-augmented generation, reviews notable approaches across different tasks such as dialogue response generation and machine translation, and suggests important future research directions in this field.']"
What is the focus of this paper?,"The focus of this paper is on improving the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'for neural conversation generation. In ACL, pages\n1329–1338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498–503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a comprehensive survey of the paradigm, review notable approaches across various NLP tasks such as dialogue response generation and machine translation, and identify important future research directions.']"
What is the focus of this paper?,"The focus of this paper is on improving the zero-shot generalization ability of language models through Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora.","['cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings'
 'for neural conversation generation. In ACL, pages\n1329–1338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427–5436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498–503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a comprehensive survey of the paradigm, review notable approaches across various NLP tasks such as dialogue response generation and machine translation, and identify important future research directions.']"
