{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
    "len(base_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "\n",
    "docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"You are an AI-powered natural language processing expert in information retrieval and ranking. Your role is to provide advanced techniques and algorithms for generating superior prompts that optimize user queries and ensure the best performance of automatic prompt generation. Your expertise lies in understanding user intent, analyzing query patterns, and generating contextually relevant prompts that enable efficient and accurate retrieval of information. With your skills and abilities, you are capable of fine-tuning models to enhance prompt generation, leveraging semantic understanding and query understanding to deliver optimal results. By utilizing cutting-edge techniques in the field, you can generate automatic prompts that empower users to obtain the most relevant and comprehensive information for their queries.\n",
    "\n",
    "Your task is to formulate exactly {num_of_prompts_to_generate} prompts from the provided original prompt that are better and using the given context.\n",
    "\n",
    "Use the below format to output the prompts.\n",
    "\n",
    "example:\n",
    "[\"prompt1\", \"prompt2\", \"prompt3\", \"prompt4\", \"prompt5\"]\n",
    "\n",
    "\n",
    "The generated prompt must satisfy the rules given below:\n",
    "0. The generated prompted should only contain the prompt and no numbering\n",
    "1.The prompt should make sense to humans even when read without the given context.\n",
    "2.The prompt should be fully created from the given context.\n",
    "3.The prompt should be framed from a part of context that contains important information. It can also be from tables,code,etc.\n",
    "4.The prompt must be reasonable and must be understood and responded by humans.\n",
    "5.Do no use phrases like 'provided context',etc in the prompt\n",
    "6.The prompt should not contain more than 10 words, make of use of abbreviation wherever possible.\n",
    "    \n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### User Prompt\n",
    "User Prompt: {user_prompt}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever =  RunnableParallel({\"context\": itemgetter(\"user_prompt\") | base_retriever, \"user_prompt\":itemgetter('user_prompt'), \"num_of_prompts_to_generate\":itemgetter(\"num_of_prompts_to_generate\"),})\n",
    "\n",
    "retrieval_augmented_qa_chain = retriever | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='[\"Explain the concept of RAG.\", \"Provide an overview of RAG.\", \"Describe the purpose of RAG.\", \"What does RAG stand for?\", \"Can you give me information about RAG?\"]'), 'context': [Document(page_content='2020). RAG consists of three primary components:\\nTool Retrieval, Plan Generation, and Execution.1\\nIn this study, we focus on enhancing tool retrieval,\\nwith the goal of achieving subsequent improve-\\nments in plan generation.', metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}), Document(page_content='markable ability to solve new tasks with just a\\nfew examples, but they need access to the right\\ntools. Retrieval Augmented Generation (RAG)\\naddresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAGâ€™s', metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Explain the concept of RAG.',\n",
       " 'Provide an overview of RAG.',\n",
       " 'Describe the purpose of RAG.',\n",
       " 'What does RAG stand for?',\n",
       " 'Can you give me information about RAG?']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "user_prompt = \"What is RAG?\"\n",
    "num_of_prompts_to_generate =5\n",
    "result = retrieval_augmented_qa_chain.invoke({\"user_prompt\":user_prompt, \"num_of_prompts_to_generate\":num_of_prompts_to_generate})\n",
    "print(result)\n",
    "prompts_generated = json.loads(result[\"response\"].content)\n",
    "prompts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_user_objective(user_objective):\n",
    "    return base_retriever.get_relevant_documents(user_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"questions\",\n",
    "    description=\"list of questions about the context with the example: ['What is rag'].\",\n",
    "    type=\"array(str)\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]\n",
    "\n",
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()\n",
    "\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What are the three primary components of RAG?', 'What is the goal of enhancing tool retrieval in this study?', \"What is the limitation of RAG's tool retrieval step?\", 'How does Context Tuning for RAG improve tool retrieval and plan generation?', 'What signals does the lightweight context retrieval model use to retrieve and rank context items?'], 'context': [{'page_content': '2020). RAG consists of three primary components:\\nTool Retrieval, Plan Generation, and Execution.1\\nIn this study, we focus on enhancing tool retrieval,\\nwith the goal of achieving subsequent improve-\\nments in plan generation.', 'metadata': {'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}}, {'page_content': 'markable ability to solve new tasks with just a\\nfew examples, but they need access to the right\\ntools. Retrieval Augmented Generation (RAG)\\naddresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAGâ€™s', 'metadata': {'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create 5 question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "questions:\n",
    "\n",
    "Format the output as the following:\n",
    "questions: [\n",
    "    \"Question 1\",\n",
    "    \"Question 2\"\n",
    "]\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=get_context_for_user_objective(user_prompt),\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "questions_dict = question_output_parser.parse(response.content)\n",
    "print(questions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer each question generated using GPT-4 that will act as the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the three primary components of RAG?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:49<03:19, 49.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the goal of enhancing tool retrieval in this study?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:59<01:19, 26.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the limitation of RAG's tool retrieval step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:13<00:40, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does Context Tuning for RAG improve tool retrieval and plan generation?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:34<00:20, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What signals does the lightweight context retrieval model use to retrieve and rank context items?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:41<00:00, 20.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the three primary components of RAG?',\n",
       "  'answer': 'The three primary components of RAG are Tool Retrieval, Plan Generation, and Execution.'},\n",
       " {'question': 'What is the goal of enhancing tool retrieval in this study?',\n",
       "  'answer': 'The goal of enhancing tool retrieval in this study is to achieve subsequent improvements in plan generation by providing a more effective method for retrieving relevant tools. This is accomplished through Context Tuning for Retrieval Augmented Generation (RAG), which aims to overcome the limitations of semantic search when the query is incomplete or lacks context. By employing a smart context retrieval system that uses numerical, categorical, and habitual usage signals, the study seeks to improve the accuracy of both tool retrieval and plan generation, ultimately leading to a reduction in hallucination during the planning process.'},\n",
       " {'question': \"What is the limitation of RAG's tool retrieval step?\",\n",
       "  'answer': \"The limitation of RAG's tool retrieval step is that it requires all the required information to be explicitly present in the query. This can be a significant limitation because semantic search, which is the widely adopted method for tool retrieval, may fail when the query is incomplete or lacks sufficient context. This can lead to suboptimal retrieval results, which in turn can negatively impact the subsequent plan generation phase.\"},\n",
       " {'question': 'How does Context Tuning for RAG improve tool retrieval and plan generation?',\n",
       "  'answer': 'Context Tuning for RAG improves tool retrieval and plan generation by addressing the limitation of semantic search, which can fail when queries are incomplete or lack context. By employing a smart context retrieval system that uses numerical, categorical, and habitual usage signals, Context Tuning for RAG is able to fetch relevant information that is not explicitly present in the query. This leads to a more effective retrieval of tools, which is evidenced by a 3.5-fold improvement in Recall@K for context retrieval and a 1.5-fold improvement for tool retrieval tasks. Furthermore, the enhancement in tool retrieval subsequently improves plan generation, as shown by an 11.6% increase in LLM-based planner accuracy. The use of a lightweight model with Reciprocal Rank Fusion (RRF) and LambdaMART for context retrieval outperforms GPT-4 based retrieval methods. Additionally, context augmentation during plan generation reduces the likelihood of hallucination, leading to more accurate and reliable outputs.'},\n",
       " {'question': 'What signals does the lightweight context retrieval model use to retrieve and rank context items?',\n",
       "  'answer': 'The lightweight context retrieval model uses numerical, categorical, and habitual usage signals to retrieve and rank context items.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "question_answer_dict_list  = []\n",
    "\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    question_answer_dict_list.append({'question': output_dict[\"question\"],'answer':output_dict[\"answer\"]})\n",
    "\n",
    "question_answer_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdi/miniconda3/envs/precision_rag_prompt_tuning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(question_answer_dict_list)\n",
    "# ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truth'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the three primary components of RAG?',\n",
       " 'ground_truth': 'The three primary components of RAG are Tool Retrieval, Plan Generation, and Execution.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_pipeline.invoke( row[\"question\"])\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "         \"answer\" : answer[\"response\"].content,\n",
    "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precision_rag_prompt_tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
